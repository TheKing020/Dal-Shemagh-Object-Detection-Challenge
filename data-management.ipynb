{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "115b47cf",
   "metadata": {
    "_cell_guid": "b5f10e99-dca3-4663-843f-4b0118e254b0",
    "_uuid": "90b4933c-97a3-44e6-970a-249fad732006",
    "collapsed": false,
    "execution": {
     "iopub.execute_input": "2026-02-13T17:59:18.405060Z",
     "iopub.status.busy": "2026-02-13T17:59:18.404778Z",
     "iopub.status.idle": "2026-02-13T17:59:28.279389Z",
     "shell.execute_reply": "2026-02-13T17:59:28.278247Z"
    },
    "jupyter": {
     "outputs_hidden": false
    },
    "papermill": {
     "duration": 9.881645,
     "end_time": "2026-02-13T17:59:28.281450",
     "exception": false,
     "start_time": "2026-02-13T17:59:18.399805",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CSV loaded: 651 entries\n",
      "Images directory: /kaggle/input/datasets/justforfun44/dal-shemagh/dal-shemagh-detection-challenge/images/train\n",
      "Labels directory: /kaggle/input/datasets/justforfun44/dal-shemagh/dal-shemagh-detection-challenge/labels/train\n",
      "\n",
      "Found 651 images\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Analyzing images: 100%|██████████| 651/651 [00:02<00:00, 236.39it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "STRATIFIED SPLIT DISTRIBUTION:\n",
      "============================================================\n",
      "rp0_h0_s0_none                 | Total: 203 | Train: 163 | Val:  40\n",
      "rp0_h0_s1_few                  | Total:  98 | Train:  79 | Val:  19\n",
      "rp0_h0_s1_many                 | Total:   2 | Train:   1 | Val:   1\n",
      "rp0_h1_s0_few                  | Total: 281 | Train: 225 | Val:  56\n",
      "rp0_h1_s0_many                 | Total:  13 | Train:  11 | Val:   2\n",
      "rp0_h1_s1_few                  | Total:  31 | Train:  25 | Val:   6\n",
      "rp0_h1_s1_many                 | Total:  18 | Train:  15 | Val:   3\n",
      "rp1_h1_s1_few                  | Total:   3 | Train:   2 | Val:   1\n",
      "rp1_h1_s1_many                 | Total:   2 | Train:   1 | Val:   1\n",
      "============================================================\n",
      "Total images: 651\n",
      "Train images: 522\n",
      "Val images: 129\n",
      "Split ratio: 19.8% validation\n",
      "============================================================\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing train: 100%|██████████| 522/522 [00:04<00:00, 116.97it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ train COCO JSON saved at: /kaggle/working/data/train/_annotations.coco.json\n",
      "   Images: 522\n",
      "   Annotations: 489\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing val: 100%|██████████| 129/129 [00:01<00:00, 91.26it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ val COCO JSON saved at: /kaggle/working/data/val/_annotations.coco.json\n",
      "   Images: 129\n",
      "   Annotations: 122\n",
      "\n",
      "============================================================\n",
      "FINAL DIRECTORY STRUCTURE:\n",
      "============================================================\n",
      "\n",
      "/kaggle/working/data/\n",
      "├── train/\n",
      "│   ├── _annotations.coco.json\n",
      "│   └── [images]\n",
      "└── val/\n",
      "    ├── _annotations.coco.json\n",
      "    └── [images]\n",
      "\n",
      "Train images: 522\n",
      "Val images: 129\n",
      "\n",
      "✅ Dataset ready for MMDetection training!\n",
      "\n",
      "Note: The original 'test' folder has no labels, so we created\n",
      "a validation set from the training data (80/20 split).\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "import shutil\n",
    "import pandas as pd\n",
    "from PIL import Image\n",
    "from tqdm import tqdm\n",
    "from collections import defaultdict\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "DATASET_PATH = \"/kaggle/input/datasets/kagglertw/dal-shemagh/dal-shemagh-detection-challenge\"\n",
    "IMAGES_DIR = os.path.join(DATASET_PATH, \"images/train\")  \n",
    "LABELS_DIR = os.path.join(DATASET_PATH, \"labels/train\")  \n",
    "CSV_PATH = os.path.join(DATASET_PATH, \"train_labels.csv\")\n",
    "\n",
    "OUTPUT_DIR = \"/kaggle/working/data\"\n",
    "TRAIN_DIR = os.path.join(OUTPUT_DIR, \"train\")\n",
    "VAL_DIR = os.path.join(OUTPUT_DIR, \"val\")  \n",
    "\n",
    "\n",
    "os.makedirs(TRAIN_DIR, exist_ok=True)\n",
    "os.makedirs(VAL_DIR, exist_ok=True)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "df = pd.read_csv(CSV_PATH)\n",
    "right_place_dict = dict(zip(df[\"filename\"], df[\"right_place\"]))\n",
    "\n",
    "print(f\"CSV loaded: {len(df)} entries\")\n",
    "print(f\"Images directory: {IMAGES_DIR}\")\n",
    "print(f\"Labels directory: {LABELS_DIR}\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "image_files = sorted([f for f in os.listdir(IMAGES_DIR) if f.endswith(\".jpg\")])\n",
    "print(f\"\\nFound {len(image_files)} images\\n\")\n",
    "\n",
    "image_info = []\n",
    "\n",
    "for img_file in tqdm(image_files, desc=\"Analyzing images\"):\n",
    "    label_path = os.path.join(LABELS_DIR, img_file.replace(\".jpg\", \".txt\"))\n",
    "    \n",
    "    has_head = False\n",
    "    has_shemagh = False\n",
    "    num_objects = 0\n",
    "    \n",
    "    if os.path.exists(label_path):\n",
    "        with open(label_path, \"r\") as f:\n",
    "            lines = f.readlines()\n",
    "        \n",
    "        for line in lines:\n",
    "            parts = line.strip().split()\n",
    "            if len(parts) == 5:\n",
    "                class_id = int(float(parts[0]))\n",
    "                num_objects += 1\n",
    "                if class_id == 0:\n",
    "                    has_head = True\n",
    "                elif class_id == 1:\n",
    "                    has_shemagh = True\n",
    "    \n",
    "    right_place = right_place_dict.get(img_file, False)\n",
    "    \n",
    "    \n",
    "    obj_bucket = \"none\" if num_objects == 0 else (\"few\" if num_objects <= 2 else \"many\")\n",
    "    strat_key = f\"rp{int(right_place)}_h{int(has_head)}_s{int(has_shemagh)}_{obj_bucket}\"\n",
    "    \n",
    "    image_info.append({\n",
    "        \"filename\": img_file,\n",
    "        \"right_place\": right_place,\n",
    "        \"has_head\": has_head,\n",
    "        \"has_shemagh\": has_shemagh,\n",
    "        \"num_objects\": num_objects,\n",
    "        \"strat_key\": strat_key\n",
    "    })\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "strat_groups = defaultdict(list)\n",
    "for info in image_info:\n",
    "    strat_groups[info[\"strat_key\"]].append(info[\"filename\"])\n",
    "\n",
    "train_files = []\n",
    "val_files = []\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"STRATIFIED SPLIT DISTRIBUTION:\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "for key, files in sorted(strat_groups.items()):\n",
    "    n_total = len(files)\n",
    "    n_val = max(1, int(n_total * 0.2))  \n",
    "    n_train = n_total - n_val\n",
    "    \n",
    "    \n",
    "    np.random.seed(42)\n",
    "    shuffled = np.random.permutation(files).tolist()\n",
    "    \n",
    "    train_files.extend(shuffled[:n_train])\n",
    "    val_files.extend(shuffled[n_train:])\n",
    "    \n",
    "    print(f\"{key:30s} | Total: {n_total:3d} | Train: {n_train:3d} | Val: {n_val:3d}\")\n",
    "\n",
    "print(\"=\"*60)\n",
    "print(f\"Total images: {len(image_files)}\")\n",
    "print(f\"Train images: {len(train_files)}\")\n",
    "print(f\"Val images: {len(val_files)}\")\n",
    "print(f\"Split ratio: {len(val_files)/len(image_files)*100:.1f}% validation\")\n",
    "print(\"=\"*60 + \"\\n\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "categories = [\n",
    "    {\"id\": 0, \"name\": \"head\"},\n",
    "    {\"id\": 1, \"name\": \"shemagh\"},\n",
    "]\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def create_coco_json(image_list, split_name):\n",
    "    \n",
    "    coco_output = {\n",
    "        \"images\": [],\n",
    "        \"annotations\": [],\n",
    "        \"categories\": categories,\n",
    "    }\n",
    "    \n",
    "    annotation_id = 0\n",
    "    \n",
    "    output_img_dir = TRAIN_DIR if split_name == \"train\" else VAL_DIR\n",
    "    \n",
    "    for image_id, img_file in enumerate(tqdm(image_list, desc=f\"Processing {split_name}\")):\n",
    "        \n",
    "        img_path = os.path.join(IMAGES_DIR, img_file)\n",
    "        label_path = os.path.join(LABELS_DIR, img_file.replace(\".jpg\", \".txt\"))\n",
    "        \n",
    "        \n",
    "        with Image.open(img_path) as img:\n",
    "            width, height = img.size\n",
    "        \n",
    "        \n",
    "        coco_output[\"images\"].append({\n",
    "            \"id\": image_id,\n",
    "            \"file_name\": img_file,\n",
    "            \"width\": width,\n",
    "            \"height\": height,\n",
    "            \"right_place\": bool(right_place_dict.get(img_file, False))\n",
    "        })\n",
    "        \n",
    "        \n",
    "        shutil.copy(img_path, os.path.join(output_img_dir, img_file))\n",
    "        \n",
    "        \n",
    "        if os.path.exists(label_path):\n",
    "            with open(label_path, \"r\") as f:\n",
    "                lines = f.readlines()\n",
    "            \n",
    "            for line in lines:\n",
    "                parts = line.strip().split()\n",
    "                \n",
    "                if len(parts) != 5:\n",
    "                    continue\n",
    "                \n",
    "                class_id, x_center, y_center, w, h = map(float, parts)\n",
    "                \n",
    "                \n",
    "                x_center *= width\n",
    "                y_center *= height\n",
    "                w *= width\n",
    "                h *= height\n",
    "                \n",
    "                x_min = x_center - (w / 2)\n",
    "                y_min = y_center - (h / 2)\n",
    "                \n",
    "                coco_output[\"annotations\"].append({\n",
    "                    \"id\": annotation_id,\n",
    "                    \"image_id\": image_id,\n",
    "                    \"category_id\": int(class_id),\n",
    "                    \"bbox\": [x_min, y_min, w, h],\n",
    "                    \"area\": w * h,\n",
    "                    \"iscrowd\": 0\n",
    "                })\n",
    "                \n",
    "                annotation_id += 1\n",
    "    \n",
    "    \n",
    "    json_path = os.path.join(output_img_dir, \"_annotations.coco.json\")\n",
    "    with open(json_path, \"w\") as f:\n",
    "        json.dump(coco_output, f, indent=2)\n",
    "    \n",
    "    print(f\"✅ {split_name} COCO JSON saved at: {json_path}\")\n",
    "    print(f\"   Images: {len(coco_output['images'])}\")\n",
    "    print(f\"   Annotations: {len(coco_output['annotations'])}\\n\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "create_coco_json(train_files, \"train\")\n",
    "create_coco_json(val_files, \"val\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"FINAL DIRECTORY STRUCTURE:\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"\\n{OUTPUT_DIR}/\")\n",
    "print(f\"├── train/\")\n",
    "print(f\"│   ├── _annotations.coco.json\")\n",
    "print(f\"│   └── [images]\")\n",
    "print(f\"└── val/\")\n",
    "print(f\"    ├── _annotations.coco.json\")\n",
    "print(f\"    └── [images]\\n\")\n",
    "\n",
    "print(f\"Train images: {len(os.listdir(TRAIN_DIR)) - 1}\")\n",
    "print(f\"Val images: {len(os.listdir(VAL_DIR)) - 1}\")\n",
    "\n",
    "print(\"\\n✅ Dataset ready for MMDetection training!\")\n",
    "print(f\"\\nNote: The original 'test' folder has no labels, so we created\")\n",
    "print(f\"a validation set from the training data (80/20 split).\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b026e836",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-13T17:59:28.296059Z",
     "iopub.status.busy": "2026-02-13T17:59:28.295236Z",
     "iopub.status.idle": "2026-02-13T17:59:28.420596Z",
     "shell.execute_reply": "2026-02-13T17:59:28.419508Z"
    },
    "papermill": {
     "duration": 0.134931,
     "end_time": "2026-02-13T17:59:28.422898",
     "exception": false,
     "start_time": "2026-02-13T17:59:28.287967",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "!mkdir /kaggle/working/data/test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a785c67",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-13T17:59:28.438885Z",
     "iopub.status.busy": "2026-02-13T17:59:28.438494Z",
     "iopub.status.idle": "2026-02-13T17:59:33.938815Z",
     "shell.execute_reply": "2026-02-13T17:59:33.937689Z"
    },
    "papermill": {
     "duration": 5.511342,
     "end_time": "2026-02-13T17:59:33.941032",
     "exception": false,
     "start_time": "2026-02-13T17:59:28.429690",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "!cp -r /kaggle/input/datasets/kagglertw/dal-shemagh/dal-shemagh-detection-challenge/images/test/* /kaggle/working/data/test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d0d8d151",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-13T17:59:33.955061Z",
     "iopub.status.busy": "2026-02-13T17:59:33.954720Z",
     "iopub.status.idle": "2026-02-13T17:59:33.960447Z",
     "shell.execute_reply": "2026-02-13T17:59:33.959631Z"
    },
    "papermill": {
     "duration": 0.015404,
     "end_time": "2026-02-13T17:59:33.962261",
     "exception": false,
     "start_time": "2026-02-13T17:59:33.946857",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def count_png_files(folder_path):\n",
    "    count = 0\n",
    "    for file in os.listdir(folder_path):\n",
    "        if file.lower().endswith(\".jpg\"):\n",
    "            count += 1\n",
    "    return count\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3e176603",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-13T17:59:33.976586Z",
     "iopub.status.busy": "2026-02-13T17:59:33.976068Z",
     "iopub.status.idle": "2026-02-13T17:59:33.982339Z",
     "shell.execute_reply": "2026-02-13T17:59:33.981313Z"
    },
    "papermill": {
     "duration": 0.015576,
     "end_time": "2026-02-13T17:59:33.984369",
     "exception": false,
     "start_time": "2026-02-13T17:59:33.968793",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "842\n"
     ]
    }
   ],
   "source": [
    "print(count_png_files(\"/kaggle/working/data/test\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a21a3089",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-13T17:59:33.998755Z",
     "iopub.status.busy": "2026-02-13T17:59:33.998115Z",
     "iopub.status.idle": "2026-02-13T17:59:34.004320Z",
     "shell.execute_reply": "2026-02-13T17:59:34.003183Z"
    },
    "papermill": {
     "duration": 0.015812,
     "end_time": "2026-02-13T17:59:34.006321",
     "exception": false,
     "start_time": "2026-02-13T17:59:33.990509",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "522\n"
     ]
    }
   ],
   "source": [
    "print(count_png_files(\"/kaggle/working/data/train\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6a859c88",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-13T17:59:34.020933Z",
     "iopub.status.busy": "2026-02-13T17:59:34.019813Z",
     "iopub.status.idle": "2026-02-13T17:59:34.025649Z",
     "shell.execute_reply": "2026-02-13T17:59:34.024774Z"
    },
    "papermill": {
     "duration": 0.014798,
     "end_time": "2026-02-13T17:59:34.027347",
     "exception": false,
     "start_time": "2026-02-13T17:59:34.012549",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "129\n"
     ]
    }
   ],
   "source": [
    "print(count_png_files(\"/kaggle/working/data/val\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be0f8ddf",
   "metadata": {
    "papermill": {
     "duration": 0.00669,
     "end_time": "2026-02-13T17:59:34.040351",
     "exception": false,
     "start_time": "2026-02-13T17:59:34.033661",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "none",
   "dataSources": [
    {
     "datasetId": 9485667,
     "sourceId": 14831476,
     "sourceType": "datasetVersion"
    },
    {
     "sourceId": 297041495,
     "sourceType": "kernelVersion"
    }
   ],
   "dockerImageVersionId": 31259,
   "isGpuEnabled": false,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 19.582996,
   "end_time": "2026-02-13T17:59:34.568030",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2026-02-13T17:59:14.985034",
   "version": "2.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
