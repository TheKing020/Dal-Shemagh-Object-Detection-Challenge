{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96e0fd77",
   "metadata": {
    "_cell_guid": "b5f10e99-dca3-4663-843f-4b0118e254b0",
    "_uuid": "90b4933c-97a3-44e6-970a-249fad732006",
    "collapsed": false,
    "execution": {
     "iopub.execute_input": "2026-02-16T17:52:03.402382Z",
     "iopub.status.busy": "2026-02-16T17:52:03.402057Z",
     "iopub.status.idle": "2026-02-16T17:52:33.015372Z",
     "shell.execute_reply": "2026-02-16T17:52:33.014058Z"
    },
    "jupyter": {
     "outputs_hidden": false
    },
    "papermill": {
     "duration": 29.620616,
     "end_time": "2026-02-16T17:52:33.017505",
     "exception": false,
     "start_time": "2026-02-16T17:52:03.396889",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CSV loaded: 651 entries\n",
      "Images directory: /kaggle/input/datasets/justforfun44/dal-shemagh/dal-shemagh-detection-challenge/images/train\n",
      "Labels directory: /kaggle/input/datasets/justforfun44/dal-shemagh/dal-shemagh-detection-challenge/labels/train\n",
      "\n",
      "Found 651 images\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Analyzing images: 100%|██████████| 651/651 [00:05<00:00, 122.41it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "STRATIFIED 5-FOLD DISTRIBUTION:\n",
      "================================================================================\n",
      "Stratification Key                  | Total | Fold Distribution\n",
      "--------------------------------------------------------------------------------\n",
      "rp0_h0_s0_none                      |   203 | F0:41 | F1:41 | F2:41 | F3:40 | F4:40\n",
      "rp0_h0_s1_few                       |    98 | F0:20 | F1:20 | F2:20 | F3:19 | F4:19\n",
      "rp0_h0_s1_many                      |     2 | F0:1 | F1:1 | F2:0 | F3:0 | F4:0\n",
      "rp0_h1_s0_few                       |   281 | F0:57 | F1:56 | F2:56 | F3:56 | F4:56\n",
      "rp0_h1_s0_many                      |    13 | F0:3 | F1:3 | F2:3 | F3:2 | F4:2\n",
      "rp0_h1_s1_few                       |    31 | F0:7 | F1:6 | F2:6 | F3:6 | F4:6\n",
      "rp0_h1_s1_many                      |    18 | F0:4 | F1:4 | F2:4 | F3:3 | F4:3\n",
      "rp1_h1_s1_few                       |     3 | F0:1 | F1:1 | F2:1 | F3:0 | F4:0\n",
      "rp1_h1_s1_many                      |     2 | F0:1 | F1:1 | F2:0 | F3:0 | F4:0\n",
      "================================================================================\n",
      "\n",
      "FOLD SIZES:\n",
      "  Fold 0: 135 images\n",
      "  Fold 1: 133 images\n",
      "  Fold 2: 131 images\n",
      "  Fold 3: 126 images\n",
      "  Fold 4: 126 images\n",
      "\n",
      "================================================================================\n",
      "CREATING 5-FOLD CROSS-VALIDATION SPLITS\n",
      "================================================================================\n",
      "\n",
      "Processing Fold 0...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                        \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  ✅ Fold 0: Train=516 imgs (478 anns) | Val=135 imgs (133 anns)\n",
      "\n",
      "Processing Fold 1...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                         \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  ✅ Fold 1: Train=518 imgs (479 anns) | Val=133 imgs (132 anns)\n",
      "\n",
      "Processing Fold 2...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                         \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  ✅ Fold 2: Train=520 imgs (488 anns) | Val=131 imgs (123 anns)\n",
      "\n",
      "Processing Fold 3...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                        \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  ✅ Fold 3: Train=525 imgs (500 anns) | Val=126 imgs (111 anns)\n",
      "\n",
      "Processing Fold 4...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                        \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  ✅ Fold 4: Train=525 imgs (499 anns) | Val=126 imgs (112 anns)\n",
      "\n",
      "Copying test data...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  test images: 100%|██████████| 842/842 [00:08<00:00, 96.10it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  ✅ Test: 842 images\n",
      "\n",
      "================================================================================\n",
      "FINAL DIRECTORY STRUCTURE:\n",
      "================================================================================\n",
      "\n",
      "/kaggle/working/data_5fold/\n",
      "├── fold_0/\n",
      "│   ├── train/\n",
      "│   │   ├── _annotations.coco.json\n",
      "│   │   └── [images]\n",
      "│   └── val/\n",
      "│       ├── _annotations.coco.json\n",
      "│       └── [images]\n",
      "├── fold_1/\n",
      "│   ├── ...\n",
      "├── fold_2/\n",
      "│   ├── ...\n",
      "├── fold_3/\n",
      "│   ├── ...\n",
      "├── fold_4/\n",
      "│   ├── ...\n",
      "└── test/\n",
      "    └── [images]\n",
      "\n",
      "================================================================================\n",
      "SUMMARY\n",
      "================================================================================\n",
      "Fold 0: Train=516 | Val=135 | Total=651\n",
      "Fold 1: Train=518 | Val=133 | Total=651\n",
      "Fold 2: Train=520 | Val=131 | Total=651\n",
      "Fold 3: Train=525 | Val=126 | Total=651\n",
      "Fold 4: Train=525 | Val=126 | Total=651\n",
      "================================================================================\n",
      "\n",
      "✅ 5-Fold Cross-Validation dataset ready for MMDetection training!\n",
      "\n",
      "To train on a specific fold, use the paths:\n",
      "  Train: /kaggle/working/data_5fold/fold_X/train\n",
      "  Val:   /kaggle/working/data_5fold/fold_X/val\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "import shutil\n",
    "import pandas as pd\n",
    "from PIL import Image\n",
    "from tqdm import tqdm\n",
    "from collections import defaultdict\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "DATASET_PATH = \"/kaggle/input/datasets/kagglertw/dal-shemagh/dal-shemagh-detection-challenge\"\n",
    "IMAGES_DIR = os.path.join(DATASET_PATH, \"images/train\")\n",
    "LABELS_DIR = os.path.join(DATASET_PATH, \"labels/train\")\n",
    "CSV_PATH = os.path.join(DATASET_PATH, \"train_labels.csv\")\n",
    "\n",
    "OUTPUT_DIR = \"/kaggle/working/data_5fold\"\n",
    "N_FOLDS = 5\n",
    "\n",
    "\n",
    "os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "df = pd.read_csv(CSV_PATH)\n",
    "right_place_dict = dict(zip(df[\"filename\"], df[\"right_place\"]))\n",
    "\n",
    "print(f\"CSV loaded: {len(df)} entries\")\n",
    "print(f\"Images directory: {IMAGES_DIR}\")\n",
    "print(f\"Labels directory: {LABELS_DIR}\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "image_files = sorted([f for f in os.listdir(IMAGES_DIR) if f.endswith(\".jpg\")])\n",
    "print(f\"\\nFound {len(image_files)} images\\n\")\n",
    "\n",
    "image_info = []\n",
    "\n",
    "for img_file in tqdm(image_files, desc=\"Analyzing images\"):\n",
    "    label_path = os.path.join(LABELS_DIR, img_file.replace(\".jpg\", \".txt\"))\n",
    "    \n",
    "    has_head = False\n",
    "    has_shemagh = False\n",
    "    num_objects = 0\n",
    "    \n",
    "    if os.path.exists(label_path):\n",
    "        with open(label_path, \"r\") as f:\n",
    "            lines = f.readlines()\n",
    "        \n",
    "        for line in lines:\n",
    "            parts = line.strip().split()\n",
    "            if len(parts) == 5:\n",
    "                class_id = int(float(parts[0]))\n",
    "                num_objects += 1\n",
    "                if class_id == 0:\n",
    "                    has_head = True\n",
    "                elif class_id == 1:\n",
    "                    has_shemagh = True\n",
    "    \n",
    "    right_place = right_place_dict.get(img_file, False)\n",
    "    \n",
    "    \n",
    "    obj_bucket = \"none\" if num_objects == 0 else (\"few\" if num_objects <= 2 else \"many\")\n",
    "    strat_key = f\"rp{int(right_place)}_h{int(has_head)}_s{int(has_shemagh)}_{obj_bucket}\"\n",
    "    \n",
    "    image_info.append({\n",
    "        \"filename\": img_file,\n",
    "        \"right_place\": right_place,\n",
    "        \"has_head\": has_head,\n",
    "        \"has_shemagh\": has_shemagh,\n",
    "        \"num_objects\": num_objects,\n",
    "        \"strat_key\": strat_key\n",
    "    })\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "strat_groups = defaultdict(list)\n",
    "for info in image_info:\n",
    "    strat_groups[info[\"strat_key\"]].append(info[\"filename\"])\n",
    "\n",
    "\n",
    "fold_assignments = {}\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"STRATIFIED 5-FOLD DISTRIBUTION:\")\n",
    "print(\"=\"*80)\n",
    "print(f\"{'Stratification Key':<35} | {'Total':>5} | Fold Distribution\")\n",
    "print(\"-\"*80)\n",
    "\n",
    "np.random.seed(42)\n",
    "\n",
    "for key, files in sorted(strat_groups.items()):\n",
    "    n_total = len(files)\n",
    "    \n",
    "    \n",
    "    shuffled = np.random.permutation(files).tolist()\n",
    "    \n",
    "    \n",
    "    fold_counts = [0] * N_FOLDS\n",
    "    fold_dist = []\n",
    "    \n",
    "    for idx, file in enumerate(shuffled):\n",
    "        fold_id = idx % N_FOLDS\n",
    "        fold_assignments[file] = fold_id\n",
    "        fold_counts[fold_id] += 1\n",
    "    \n",
    "    fold_dist_str = \" | \".join([f\"F{i}:{fold_counts[i]}\" for i in range(N_FOLDS)])\n",
    "    print(f\"{key:<35} | {n_total:>5} | {fold_dist_str}\")\n",
    "\n",
    "print(\"=\"*80 + \"\\n\")\n",
    "\n",
    "\n",
    "fold_totals = [0] * N_FOLDS\n",
    "for file, fold_id in fold_assignments.items():\n",
    "    fold_totals[fold_id] += 1\n",
    "\n",
    "print(\"FOLD SIZES:\")\n",
    "for i in range(N_FOLDS):\n",
    "    print(f\"  Fold {i}: {fold_totals[i]} images\")\n",
    "print()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "categories = [\n",
    "    {\"id\": 0, \"name\": \"head\"},\n",
    "    {\"id\": 1, \"name\": \"shemagh\"},\n",
    "]\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def create_coco_json(image_list, output_dir, split_name):\n",
    "    \n",
    "    coco_output = {\n",
    "        \"images\": [],\n",
    "        \"annotations\": [],\n",
    "        \"categories\": categories,\n",
    "    }\n",
    "    \n",
    "    annotation_id = 0\n",
    "    \n",
    "    for image_id, img_file in enumerate(tqdm(image_list, desc=f\"  {split_name}\", leave=False)):\n",
    "        \n",
    "        img_path = os.path.join(IMAGES_DIR, img_file)\n",
    "        label_path = os.path.join(LABELS_DIR, img_file.replace(\".jpg\", \".txt\"))\n",
    "        \n",
    "        \n",
    "        with Image.open(img_path) as img:\n",
    "            width, height = img.size\n",
    "        \n",
    "        \n",
    "        coco_output[\"images\"].append({\n",
    "            \"id\": image_id,\n",
    "            \"file_name\": img_file,\n",
    "            \"width\": width,\n",
    "            \"height\": height,\n",
    "            \"right_place\": bool(right_place_dict.get(img_file, False))\n",
    "        })\n",
    "        \n",
    "        \n",
    "        shutil.copy(img_path, os.path.join(output_dir, img_file))\n",
    "        \n",
    "        \n",
    "        if os.path.exists(label_path):\n",
    "            with open(label_path, \"r\") as f:\n",
    "                lines = f.readlines()\n",
    "            \n",
    "            for line in lines:\n",
    "                parts = line.strip().split()\n",
    "                \n",
    "                if len(parts) != 5:\n",
    "                    continue\n",
    "                \n",
    "                class_id, x_center, y_center, w, h = map(float, parts)\n",
    "                \n",
    "                \n",
    "                x_center *= width\n",
    "                y_center *= height\n",
    "                w *= width\n",
    "                h *= height\n",
    "                \n",
    "                x_min = x_center - (w / 2)\n",
    "                y_min = y_center - (h / 2)\n",
    "                \n",
    "                coco_output[\"annotations\"].append({\n",
    "                    \"id\": annotation_id,\n",
    "                    \"image_id\": image_id,\n",
    "                    \"category_id\": int(class_id),\n",
    "                    \"bbox\": [x_min, y_min, w, h],\n",
    "                    \"area\": w * h,\n",
    "                    \"iscrowd\": 0\n",
    "                })\n",
    "                \n",
    "                annotation_id += 1\n",
    "    \n",
    "    \n",
    "    json_path = os.path.join(output_dir, \"_annotations.coco.json\")\n",
    "    with open(json_path, \"w\") as f:\n",
    "        json.dump(coco_output, f, indent=2)\n",
    "    \n",
    "    return len(coco_output['images']), len(coco_output['annotations'])\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "print(\"=\"*80)\n",
    "print(\"CREATING 5-FOLD CROSS-VALIDATION SPLITS\")\n",
    "print(\"=\"*80 + \"\\n\")\n",
    "\n",
    "for fold_id in range(N_FOLDS):\n",
    "    print(f\"Processing Fold {fold_id}...\")\n",
    "    \n",
    "    \n",
    "    fold_dir = os.path.join(OUTPUT_DIR, f\"fold_{fold_id}\")\n",
    "    train_dir = os.path.join(fold_dir, \"train\")\n",
    "    val_dir = os.path.join(fold_dir, \"val\")\n",
    "    \n",
    "    os.makedirs(train_dir, exist_ok=True)\n",
    "    os.makedirs(val_dir, exist_ok=True)\n",
    "    \n",
    "    \n",
    "    train_files = []\n",
    "    val_files = []\n",
    "    \n",
    "    for file, assigned_fold in fold_assignments.items():\n",
    "        if assigned_fold == fold_id:\n",
    "            val_files.append(file)\n",
    "        else:\n",
    "            train_files.append(file)\n",
    "    \n",
    "    \n",
    "    n_train_imgs, n_train_anns = create_coco_json(train_files, train_dir, \"train\")\n",
    "    n_val_imgs, n_val_anns = create_coco_json(val_files, val_dir, \"val\")\n",
    "    \n",
    "    print(f\"  ✅ Fold {fold_id}: Train={n_train_imgs} imgs ({n_train_anns} anns) | \"\n",
    "          f\"Val={n_val_imgs} imgs ({n_val_anns} anns)\")\n",
    "    print()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "print(\"Copying test data...\")\n",
    "test_dir = os.path.join(OUTPUT_DIR, \"test\")\n",
    "os.makedirs(test_dir, exist_ok=True)\n",
    "\n",
    "test_src = os.path.join(DATASET_PATH, \"images/test\")\n",
    "if os.path.exists(test_src):\n",
    "    for file in tqdm(os.listdir(test_src), desc=\"  test images\"):\n",
    "        if file.lower().endswith(\".jpg\"):\n",
    "            shutil.copy(os.path.join(test_src, file), os.path.join(test_dir, file))\n",
    "    n_test = len([f for f in os.listdir(test_dir) if f.endswith(\".jpg\")])\n",
    "    print(f\"  ✅ Test: {n_test} images\\n\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "print(\"=\" * 80)\n",
    "print(\"FINAL DIRECTORY STRUCTURE:\")\n",
    "print(\"=\" * 80)\n",
    "print(f\"\\n{OUTPUT_DIR}/\")\n",
    "print(f\"├── fold_0/\")\n",
    "print(f\"│   ├── train/\")\n",
    "print(f\"│   │   ├── _annotations.coco.json\")\n",
    "print(f\"│   │   └── [images]\")\n",
    "print(f\"│   └── val/\")\n",
    "print(f\"│       ├── _annotations.coco.json\")\n",
    "print(f\"│       └── [images]\")\n",
    "print(f\"├── fold_1/\")\n",
    "print(f\"│   ├── ...\")\n",
    "print(f\"├── fold_2/\")\n",
    "print(f\"│   ├── ...\")\n",
    "print(f\"├── fold_3/\")\n",
    "print(f\"│   ├── ...\")\n",
    "print(f\"├── fold_4/\")\n",
    "print(f\"│   ├── ...\")\n",
    "print(f\"└── test/\")\n",
    "print(f\"    └── [images]\\n\")\n",
    "\n",
    "print(\"=\"*80)\n",
    "print(\"SUMMARY\")\n",
    "print(\"=\"*80)\n",
    "for fold_id in range(N_FOLDS):\n",
    "    fold_dir = os.path.join(OUTPUT_DIR, f\"fold_{fold_id}\")\n",
    "    train_dir = os.path.join(fold_dir, \"train\")\n",
    "    val_dir = os.path.join(fold_dir, \"val\")\n",
    "    \n",
    "    n_train = len([f for f in os.listdir(train_dir) if f.endswith(\".jpg\")])\n",
    "    n_val = len([f for f in os.listdir(val_dir) if f.endswith(\".jpg\")])\n",
    "    \n",
    "    print(f\"Fold {fold_id}: Train={n_train:3d} | Val={n_val:3d} | Total={n_train+n_val}\")\n",
    "\n",
    "print(\"=\"*80)\n",
    "print(\"\\n✅ 5-Fold Cross-Validation dataset ready for MMDetection training!\")\n",
    "print(f\"\\nTo train on a specific fold, use the paths:\")\n",
    "print(f\"  Train: {OUTPUT_DIR}/fold_X/train\")\n",
    "print(f\"  Val:   {OUTPUT_DIR}/fold_X/val\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4aa76523",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-16T17:52:33.052647Z",
     "iopub.status.busy": "2026-02-16T17:52:33.052243Z",
     "iopub.status.idle": "2026-02-16T17:52:33.188150Z",
     "shell.execute_reply": "2026-02-16T17:52:33.186582Z"
    },
    "papermill": {
     "duration": 0.15831,
     "end_time": "2026-02-16T17:52:33.191136",
     "exception": false,
     "start_time": "2026-02-16T17:52:33.032826",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mkdir: cannot create directory ‘/kaggle/working/data/test’: No such file or directory\r\n"
     ]
    }
   ],
   "source": [
    "!mkdir /kaggle/working/data/test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41215f70",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-16T17:52:33.225280Z",
     "iopub.status.busy": "2026-02-16T17:52:33.224856Z",
     "iopub.status.idle": "2026-02-16T17:52:33.367979Z",
     "shell.execute_reply": "2026-02-16T17:52:33.366719Z"
    },
    "papermill": {
     "duration": 0.163459,
     "end_time": "2026-02-16T17:52:33.370412",
     "exception": false,
     "start_time": "2026-02-16T17:52:33.206953",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cp: target '/kaggle/working/data/test' is not a directory\r\n"
     ]
    }
   ],
   "source": [
    "!cp -r /kaggle/input/datasets/kagglertw/dal-shemagh/dal-shemagh-detection-challenge/images/test/* /kaggle/working/data/test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c12afb1f",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-16T17:52:33.405289Z",
     "iopub.status.busy": "2026-02-16T17:52:33.404943Z",
     "iopub.status.idle": "2026-02-16T17:52:33.411280Z",
     "shell.execute_reply": "2026-02-16T17:52:33.410395Z"
    },
    "papermill": {
     "duration": 0.027394,
     "end_time": "2026-02-16T17:52:33.413369",
     "exception": false,
     "start_time": "2026-02-16T17:52:33.385975",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def count_png_files(folder_path):\n",
    "    count = 0\n",
    "    for file in os.listdir(folder_path):\n",
    "        if file.lower().endswith(\".jpg\"):\n",
    "            count += 1\n",
    "    return count\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e06d15c8",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-16T17:52:33.445592Z",
     "iopub.status.busy": "2026-02-16T17:52:33.444911Z",
     "iopub.status.idle": "2026-02-16T17:52:33.458488Z",
     "shell.execute_reply": "2026-02-16T17:52:33.457244Z"
    },
    "papermill": {
     "duration": 0.032017,
     "end_time": "2026-02-16T17:52:33.460312",
     "exception": true,
     "start_time": "2026-02-16T17:52:33.428295",
     "status": "failed"
    },
    "tags": []
   },
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '/kaggle/working/data/test'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_17/3977289707.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcount_png_files\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"/kaggle/working/data/test\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/tmp/ipykernel_17/631702008.py\u001b[0m in \u001b[0;36mcount_png_files\u001b[0;34m(folder_path)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mcount_png_files\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfolder_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m     \u001b[0mcount\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m     \u001b[0;32mfor\u001b[0m \u001b[0mfile\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlistdir\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfolder_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mfile\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlower\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mendswith\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\".jpg\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m             \u001b[0mcount\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/kaggle/working/data/test'"
     ]
    }
   ],
   "source": [
    "print(count_png_files(\"/kaggle/working/data/test\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a780a7ca",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "none",
   "dataSources": [
    {
     "datasetId": 9485667,
     "sourceId": 14831476,
     "sourceType": "datasetVersion"
    },
    {
     "sourceId": 297041495,
     "sourceType": "kernelVersion"
    }
   ],
   "dockerImageVersionId": 31259,
   "isGpuEnabled": false,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 34.494467,
   "end_time": "2026-02-16T17:52:34.096346",
   "environment_variables": {},
   "exception": true,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2026-02-16T17:51:59.601879",
   "version": "2.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
